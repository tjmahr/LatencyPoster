<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
</head>
<body>
<h2 id="introduction-and-rationale">Introduction and Rationale</h2>
<ul>
<li>The 2AFC looking-while-listening paradigm <span class="citation" data-cites="FernaldLWL">(LWL; Fernald, Zangl, Portillo, &amp; Marchman, 2008)</span> has become widely used to examine lexical processing in young children.</li>
<li>The speed at which children look to familiar objects when hearing the object-name at 18 months reliably predicts vocabulary size up to 8 years of age <span class="citation" data-cites="MarchmanLangOutcomes">(Marchman &amp; Fernald, 2008)</span>.
<ul>
<li>However, these reaction time measures are not easily obtained.</li>
<li>Reaction time is a measure of how quickly a child looks to a picture when its object name is presented. Therefore, reaction time can be measured only on trials where the child is <em>not</em> looking at the target picture at the onset of the target word.</li>
<li>In a 2AFC paradigm, only about 50% of trials provide reaction time data.</li>
<li>Usually, even fewer trials provide reaction time data because there are always some trials where young children are not fixating on a picture at target word onset.</li>
<li>This is a considerable problem, given the small number of trials in LWL studies (usually between 24 and 36).</li>
</ul></li>
<li>Adults can be instructed to fixate on a central orienting stimuli, but young children cannot be similarly instructed.</li>
<li>This study used an animated centering stimulus in an attempt to increase the number of trials with useable reaction times.</li>
</ul>
<h2 id="method">METHOD</h2>
<h3 id="conditions">Conditions</h3>
<p>Condition 1: No animated centering stimulus. Condition 2: Animated centering stimulus.</p>
<ul>
<li>Centering stimulus was an abstract geometric animation. It appeared onscreen after two images had been presented for 2000 ms. (<strong>Describe and maybe show example</strong>)</li>
<li>The animation looped until the child had fixated on it for 300 ms or until 8000 ms had elapsed. Then the carrier phrase was played, at target-word onset, the centering stimulus disappeared.</li>
<li>Because carrier phrase and target-word presentation were triggered by fixation to the animation, these trials incorporated <em>gaze-contingency</em> into the LWL paradigm.</li>
</ul>
<h3 id="participants">Participants</h3>
<ul>
<li>N = 25 (12 female, 13 male) in condition 1 and N = 25 (11 female, 14 male) in condition 2.</li>
<li>Participants in the two groups closely matched on the basis of age, sex, and PPVT-4 standard score.</li>
</ul>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;">Age (months)</td>
<td style="text-align: left;">EVT-2</td>
<td style="text-align: left;">PPVT-4</td>
</tr>
<tr class="even">
<td style="text-align: left;">CS1</td>
<td style="text-align: left;">39.44 (30–46)</td>
<td style="text-align: left;">62 (37–90)</td>
<td style="text-align: left;">90.2 (50–128)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">CS2</td>
<td style="text-align: left;">40.52 (31–48)</td>
<td style="text-align: left;">56.6 (27–90)</td>
<td style="text-align: left;">77.04 (45–118)</td>
</tr>
</tbody>
</table>
<h3 id="methodology">Methodology</h3>
<ul>
<li>Looking-while-listening mispronunciation paradigm <span class="citation" data-cites="SwingleyMP WhiteSubsegmental">(Swingley &amp; Aslin, 2000; White &amp; Morgan, 2008)</span></li>
<li>Experiment designed in E-Prime Professional 2.0, used to interface with Tobii T60 XL Eye-tracker.</li>
<li>Eye-tracking task presented to children as “watching a movie.”</li>
<li>Images presented onscreen: one familiar and one unfamiliar object.</li>
<li>Position counterbalanced (left-right).</li>
<li>Images normed for familiarity and unfamiliarity.</li>
</ul>
<h4 id="three-conditions">Three conditions</h4>
<ol type="1">
<li>CP: Correct pronunciation of real words</li>
<li>MP: Mispronunciations of these real words, with a one-feature change of initial consonant</li>
<li>NW: Nonwords trials presented with familiar objects not used in CP trials</li>
</ol>
<ul>
<li>Target words all CVC in the carrier phrases “See the ____!” or “Find the ____!”</li>
<li>(6 CP + 6 MP + 6 NW) * 2 repetitions + 2 other real-word familiarization trials = 38 trials</li>
<li>2 blocks of 38 trials, eye-tracker calibrated before each block.</li>
<li>Brief animation played every 6–7 trials to keep child engaged in task.</li>
</ul>
<figure>
<img src="mp_display.png" />
</figure>
<h3 id="calculation-of-latency-reaction-time">Calculation of Latency (reaction time)</h3>
<ul>
<li>Latency is the amount of time between target-word onset and the first look to target.</li>
<li>Latency calculated for CP and NW trials only.</li>
<li><p>On each trial, reaction time was calculated only if:</p>
<ol type="1">
<li>the child looked onscreen within the 50 ms after target-word onset</li>
<li>the child was not already looking at familiar object (CP trials) or at unfamiliar object (NW trials) during within 50 ms after target-word onset.</li>
</ol></li>
<li>Latency = Time of first look to target – time of first tracked look during target-word onset (0 to 50 ms)</li>
<li><p>Reaction time trimming: We excluded latencies that were less than 250 ms or greater than 2SD above the group mean.</p></li>
</ul>
<h3 id="research-questions">Research Questions</h3>
<ul>
<li>Does the use of an animated centering stimulus result in more useable latencies?
<ul>
<li>That is, are there more trials with useable latencies in condition 2 as compared to condition 1?</li>
</ul></li>
<li>Does this animated centering stimulus create additional task demands?
<ul>
<li>Do children take longer to look to the target in condition 2 relative to condition 1?</li>
<li>Does the relationship between reaction time and vocabulary size reported in the literature continue to be observed when an animated centering stimulus is used?</li>
</ul></li>
</ul>
<h2 id="results">RESULTS</h2>
<ul>
<li>As expected, children looked to familiar object in CP trials and to unfamiliar object in NW trials.</li>
</ul>
<h3 id="latency-results">Latency Results</h3>
<ul>
<li>Condition 1:
<ul>
<li>CP trials: Latencies available in 32.7% of trials (additional 4.8% trimmed)</li>
<li>NW trials: Latencies available in 30.5% of trials (additional 5.8% trimmed)</li>
</ul></li>
<li>Condition 2:
<ul>
<li>CP trials: Latencies available in 63.9% of trials (additional 8% trimmed)</li>
<li>NW trials: Latencies available in 61.3% of trials (additional 8.5% trimmed)</li>
</ul></li>
<li>Mean latencies are very similar across the two conditions and the two trial types.</li>
<li>Distributions of latencies differs across the two conditions, with condition 2 (with the centering stimuli) having a more peaky and positively skewed distribution.</li>
</ul>
<figure>
<img src="figure/unnamed-chunk-5.png" alt="Figure 2. Histograms of latencies (in ms) for condition 1 (top) and condition 2 (bottom) for CP trials." /><figcaption>Figure 2. Histograms of latencies (in ms) for condition 1 (top) and condition 2 (bottom) for CP trials.</figcaption>
</figure>
<figure>
<img src="figure/unnamed-chunk-6.png" alt="Figure 2. Histograms of latencies (in ms) for condition 1 (top) and condition 2 (bottom) for CP trials." /><figcaption>Figure 2. Histograms of latencies (in ms) for condition 1 (top) and condition 2 (bottom) for CP trials.</figcaption>
</figure>
<figure>
<img src="figure/unnamed-chunk-7.png" alt="Figure 3. Histograms of latencies (in ms) for condition 1 (top) and condition 2 (bottom) for NW trials." /><figcaption>Figure 3. Histograms of latencies (in ms) for condition 1 (top) and condition 2 (bottom) for NW trials.</figcaption>
</figure>
<h3 id="regression-analyses">Regression analyses</h3>
<ul>
<li>Do age, receptive vocabulary size, or expressive vocabulary size predict latency for either the CP or the NW trials in the two conditions?</li>
<li>We ran four separate multiple regression analyses with four dependent variables. These were mean latencies for each subject by condition (1 or 2), and by trial type (CP or NW).</li>
<li>Independent variables were age, PPVT-4 raw score (receptive vocabulary size) and EVT-2 raw score (expressive vocabulary size).</li>
</ul>
<h3 id="regression-results-condition-1">Regression Results: Condition 1</h3>
<ul>
<li>Age, trial type, and EVT-2 were significant predictors of latency.</li>
</ul>
<h3 id="regression-results-condition-2">Regression Results: Condition 2</h3>
<ul>
<li>None of the independent variables were significant predictors of latency.</li>
</ul>
<h3 id="discussion">Discussion</h3>
<ul>
<li>These results suggest that using an animated centering stimulus will yield more useable latency data.
<ul>
<li>About ??% of trials had useable latencies when a animated centering stimulus was used, as compared to ??% when it was not used.</li>
</ul></li>
<li>The fact that reaction times were not significantly different across the two conditions suggests that the animated centering stimulus does not create additional task demands.</li>
<li>As in previous research, vocabulary size was a significant predictor of latency in condition 1 without the animated centering stimulus</li>
<li>However, neither vocabulary size nor age was a significant predictor of latency when an animated centering stimulus was used.</li>
<li>This result suggests that the effect of age and vocabulary size on latency may be due, at least in part, to older children and children with larger vocabularies having better attention to task. When an animated centering stimulus is used to maintain attention, the effect of age and vocabulary size on latency is no longer observed.</li>
<li>To conclude, the use of an animated centering stimulus does not create additional task demands. Instead, it results in more useable latency data and better attention to task.</li>
</ul>
<h3 id="acknowledgements">Acknowledgements</h3>
<p>Thanks to Franzo Law II, Alissa Schneeberg, Danielle Lee, David Kaplan, Morgan Meredith, Erica Richmond, Nancy Wermuth, and other members of the Learning To Talk Laboratory for help with many aspects of this study. We also thank the children who participated and their parents.</p>
<p>This research was supported by NIDCD Grant R01-02932 to Jan Edwards, Mary Beckman, and Benjamin Munson and NICHD Grant P30-HD03352 to the Waisman Center.</p>
<h2 id="references">References</h2>
<p>Fernald, A., Zangl, R., Portillo, A. L., &amp; Marchman, V. A. (2008). Looking while listening: Using eye movements to monitor spoken language comprehension by infants and young children. In I. A. Sekerina, E. M. Fernández, &amp; H. Clahsen (Eds.), <em>Developmental Psycholinguistics: On-line Methods in Children’s Language Processing</em> (pp. 97–135). Amsterdam: John Benjamins Publishing Company.</p>
<p>Marchman, V. A., &amp; Fernald, A. (2008). Speed of word recognition and vocabulary knowledge in infancy predict cognitive and language outcomes in later childhood. <em>Developmental Science</em>, <em>11</em>(3), F9–F16. doi:10.1111/j.1467-7687.2008.00671.x</p>
<p>Swingley, D., &amp; Aslin, R. N. (2000). Spoken word recognition and lexical representation in very young children. <em>Cognition</em>, <em>76</em>(2), 147–166. doi:10.1016/s0010-0277(00)00081-0</p>
<p>White, K. S., &amp; Morgan, J. L. (2008). Sub-segmental detail in early lexical representations. <em>Journal of Memory and Language</em>, <em>59</em>(1), 114–132. doi:10.1016/j.jml.2008.03.001</p>
</body>
</html>
