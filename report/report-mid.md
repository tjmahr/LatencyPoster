# Do orienting stimuli create additional task demands in the looking-while-listening paradigm?

## Introduction
The looking-while-listening paradigm [LWL; @FernaldLWL] uses eye-tracking to study lexical comprehension in young children. In this procedure, two images are presented on a computer screen followed by a prompt to look at one of the images. The data gathered through eye-tracking not only records _where_ the child looks onscreen but _when_ the child fixates on a particular image. The latency between the onset of a speech stimulus and an appropriate change of gaze location provides a measure of how rapidly the child accesses the word's lexical representation. Reaction time is related to vocabulary size in young children and is also predictive of later language abilities [@FernaldHalfAWord; @MarchmanLangOutcomes]. 

Reaction times provide valuable information about the real-time processing of speech signal in children, but these data are not easily obtained. In a 2-alternative forced choice (2AFC) LWL paradigm, there is a 50% chance the child will be fixated on the target image at onset of the speech stimulus, so only half the trials will provide latency data. This problem is readily resolved in studies with adults by instructing participants to fixate on a central orienting image until they hear the stimulus. Unfortunately, young children cannot be similarly instructed.

In this study, we modified the LWL paradigm to include an animated centering stimulus in order to increase the number of trials with meaningful reaction time data. We hypothesized that the orienting animations will increase the "quality" of the eye-tracking data but may also introduce additional task demands that hinder response time and reduce accuracy. 

## Methods

### Participants
The participants were fifty 30-- to 48-month-old children, 25 per condition (with and without the central fixation point). All participants passed a hearing screening and had age-appropriate speech and language, according to parent report. 

### Procedure
A 2AFC mispronunciation experiment was used in condition. In this experiment, children saw pictures of a familiar and an unfamiliar object and heard three types of stimuli in a carrier phrase: familiar real words, one-feature mispronunciations (_dog_ vs. _tog_), and nonwords (/veif/).  Children also received an expressive vocabulary test (EVT-2, Williams, 2007).

### Statistical Analysis
The log-odds of looking to the familiar object was the dependent variable.  We used a growth curve model to model how childrenâ€™s eye gaze patterns were related to their expressive vocabulary size (Mirman et al., 2008)

## Results and Discussion









*** 

## Scratch area for testing things out

**Ciations**

* @BaayenRTs recommend blah blah blah.
* We try the inverse-normal distribution [@BaayenRTs],

**Embed an image!**


```r
plot(cars)
```

![Test image](figure/unnamed-chunk-1.png) 


**Tables?**

Use the `ascii` package to make a captioned Pandoc table:


```r
library(ascii)
data(esoph)
print(ascii(esoph[1:10, ]), type = "pandoc")
```

     **agegp**   **alcgp**   **tobgp**   **ncases**   **ncontrols**  
---- ----------- ----------- ----------- ------------ ---------------
1    25-34       0-39g/day   0-9g/day    0.00         40.00          
2    25-34       0-39g/day   10-19       0.00         10.00          
3    25-34       0-39g/day   20-29       0.00         6.00           
4    25-34       0-39g/day   30+         0.00         5.00           
5    25-34       40-79       0-9g/day    0.00         27.00          
6    25-34       40-79       10-19       0.00         7.00           
7    25-34       40-79       20-29       0.00         4.00           
8    25-34       40-79       30+         0.00         7.00           
9    25-34       80-119      0-9g/day    0.00         2.00           
10   25-34       80-119      10-19       0.00         1.00           
---- ----------- ----------- ----------- ------------ ---------------

Table: This is a table caption.







## References
